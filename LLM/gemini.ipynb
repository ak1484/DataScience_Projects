{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.6 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.6-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.141.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.34.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.27.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pydantic (from google-generativeai)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting tqdm (from google-generativeai)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ankit\\miniconda3\\envs\\llm\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.6->google-generativeai)\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-4.25.4-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting requests<3.0.0.dev0,>=2.18.0 (from google-api-core->google-generativeai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic->google-generativeai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic->google-generativeai)\n",
      "  Downloading pydantic_core-2.20.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ankit\\miniconda3\\envs\\llm\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai)\n",
      "  Downloading grpcio-1.65.5-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai)\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached google_generativeai-0.7.2-py3-none-any.whl (164 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.6-py3-none-any.whl (718 kB)\n",
      "Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "Downloading google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
      "Downloading protobuf-4.25.4-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading google_api_python_client-2.141.0-py2.py3-none-any.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 4.5/12.2 MB 22.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.2 MB 23.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 23.0 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Downloading pydantic_core-2.20.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 17.4 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Downloading grpcio-1.65.5-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.1/4.1 MB 24.9 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, uritemplate, tqdm, pyparsing, pydantic-core, pyasn1, protobuf, idna, grpcio, charset-normalizer, certifi, cachetools, annotated-types, rsa, requests, pydantic, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.5.0 certifi-2024.7.4 charset-normalizer-3.3.2 google-ai-generativelanguage-0.6.6 google-api-core-2.19.1 google-api-python-client-2.141.0 google-auth-2.34.0 google-auth-httplib2-0.2.0 google-generativeai-0.7.2 googleapis-common-protos-1.63.2 grpcio-1.65.5 grpcio-status-1.62.3 httplib2-0.22.0 idna-3.7 proto-plus-1.24.0 protobuf-4.25.4 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.8.2 pydantic-core-2.20.1 pyparsing-3.1.2 requests-2.32.3 rsa-4.9 tqdm-4.66.5 uritemplate-4.1.1 urllib3-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Write a story about a magic backpack.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(pdf_path, output_txt):\n",
    "    # Open the PDF file in read-binary mode\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        # Create a PdfReader object instead of PdfFileReader\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Initialize an empty string to store the text\n",
    "        text = ''\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Write the extracted text to a text file\n",
    "    with open(output_txt, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF converted to text successfully!\n"
     ]
    }
   ],
   "source": [
    "pdf_path = 'pdf/14961_2016_Judgement_06-Sep-2018.pdf'\n",
    "\n",
    "output_txt = 'pdf/14961_2016_Judgement_06-Sep-2018.txt'\n",
    "\n",
    "pdf_to_text(pdf_path, output_txt)\n",
    "\n",
    "print(\"PDF converted to text successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF converted to text successfully!\n"
     ]
    }
   ],
   "source": [
    "pdf_path = 'pdf/41070.pdf'\n",
    "\n",
    "output_txt = 'pdf/41070.txt'\n",
    "\n",
    "pdf_to_text(pdf_path, output_txt)\n",
    "\n",
    "print(\"PDF converted to text successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.7.24-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ankit\\miniconda3\\envs\\llm\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ankit\\miniconda3\\envs\\llm\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.7.24-cp310-cp310-win_amd64.whl (269 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.7.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ankit\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been cleaned and saved to pdf/41070_cleaned_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download stop words list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# File paths\n",
    "input_file = 'pdf/41070.txt'\n",
    "output_file = 'pdf/41070_cleaned_output.txt'\n",
    "\n",
    "# Read the input text file\n",
    "with open(input_file, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Write the cleaned text to a new file\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "print(f\"Text has been cleaned and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been cleaned and saved to pdf/14961_2016_Judgement_06-Sep-2018_cleaned_output.txt\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "input_file = 'pdf/14961_2016_Judgement_06-Sep-2018.txt'\n",
    "output_file = 'pdf/14961_2016_Judgement_06-Sep-2018_cleaned_output.txt'\n",
    "\n",
    "# Read the input text file\n",
    "with open(input_file, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Write the cleaned text to a new file\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "print(f\"Text has been cleaned and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barnaby was a curious boy. He loved exploring the woods behind his house, his pockets always stuffed with treasures he found: smooth stones, feathers, and the occasional lost button. But Barnaby yearned for more. He dreamed of faraway lands, of talking animals and shimmering castles. He dreamed of adventure.\n",
      "\n",
      "One day, rummaging through his attic, Barnaby stumbled upon an old, dusty backpack. It was unlike any he’d seen before, woven from leather so supple it seemed to breathe, with a single, silver buckle that glinted mysteriously. He unclipped it, and the air filled with a soft hum.\n",
      "\n",
      "As Barnaby pulled the backpack over his shoulders, a tingling sensation ran through him. He closed his eyes, and when he opened them, he was no longer in the attic. He stood on a sun-drenched beach, the air alive with the sounds of exotic birds and the salty scent of the ocean. He was on a faraway island, teeming with life.\n",
      "\n",
      "He explored the island, the backpack seemingly empty yet somehow always providing what he needed. When he needed water, he found a cool spring bubbling at his feet. When he was hungry, he found plump, juicy mangoes hanging from a tree. When he felt lonely, a friendly monkey with sparkling eyes appeared, chattering happily.\n",
      "\n",
      "Barnaby spent days on the island, living out his dreams. He learned to speak the language of the animals, rode a giant sea turtle through the waves, and even climbed a mountain that touched the clouds. The backpack, his silent companion, was always there, a source of endless wonder and adventure.\n",
      "\n",
      "One day, Barnaby felt a pang of homesickness. He longed for the familiar scent of his mother's apple pie and the sound of his father's laughter. He wished to share his adventures with them.\n",
      "\n",
      "He closed his eyes, and when he opened them, he was back in the attic, the backpack still on his shoulders. It felt lighter now, as if the weight of his adventures had been lifted.  He rushed downstairs, his heart brimming with stories. \n",
      "\n",
      "From that day on, Barnaby's life was never the same. The magic backpack remained in his attic, a silent reminder of his extraordinary adventure. But the true magic, he realized, wasn't the backpack itself, but the spirit of adventure it had awakened within him. He carried that spirit in his heart, and with it, he continued to find adventure in the most ordinary of places, in the woods behind his house, in the stories he shared with his family, and in the world waiting to be explored. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Write a story about a magic backpack.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Downloading numpy-2.1.0-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ankit\\miniconda3\\envs\\llm\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ankit\\miniconda3\\envs\\llm\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.3/11.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 28.0 MB/s eta 0:00:00\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading numpy-2.1.0-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 4.7/12.9 MB 22.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.9 MB 22.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 22.4 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, et-xmlfile, pandas, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 numpy-2.1.0 openpyxl-3.1.5 pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize the Gemini model\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config={\"response_mime_type\": \"application/json\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "# file name\n",
    "# file_name = \"pdf/14961_2016_Judgement_06-Sep-2018_cleaned_output.txt\"\n",
    "file_name = \"pdf/41070_cleaned_output.txt\"\n",
    "# Load the cleaned text file\n",
    "with open(file_name, 'r') as file:\n",
    "    text = file.read()\n",
    "# Split the text into chunks of 1000 words\n",
    "words = text.split()\n",
    "chunks = [' '.join(words[i:i + 1000]) for i in range(0, len(words), 1000)]\n",
    "print(len(chunks))\n",
    "# List to hold the data\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : [{'question': 'What is a characteristic of India?', 'answer': 'India is a very large country.'}, {'question': 'Is India a small country?', 'answer': 'No, India is a very large country.'}, {'question': 'What is the size of India compared to other countries?', 'answer': 'India is considered a very large country.'}]\n",
      "Questions and answers have been saved to qa_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Time delay in seconds\n",
    "sm = 0  # Adjust this as needed\n",
    "# Process each chunk\n",
    "chunks =[\"india is a ver large country\"]\n",
    "for chunk in chunks:\n",
    "    prompt = f\"\"\"{chunk}\n",
    "    generate 3 questions and answers from the above context.\n",
    "    Using this JSON schema:\n",
    "    Content = {{\"question\": str, \"answer\": str}}\n",
    "    Return a `list[Content]`\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        \n",
    "        # Check if the response contains valid parts\n",
    "        if response.parts:\n",
    "            # Assuming response.text is a list of dictionaries\n",
    "            output = eval(response.text)  # Convert string representation to list of dicts\n",
    "            # print('response :',output)\n",
    "        \n",
    "            # break\n",
    "            \n",
    "            # Add to data list\n",
    "            for item in output:\n",
    "                data.append({\n",
    "                    \"Question\": item[\"question\"],\n",
    "                    \"Answer\": item[\"answer\"]\n",
    "                })\n",
    "            # Wait for a random time between 1 to 3 seconds before processing the next chunk\n",
    "            delay_seconds = random.uniform(1, 3)\n",
    "            sm+=delay_seconds\n",
    "            print(sm,\" \",delay_seconds)\n",
    "            time.sleep(delay_seconds)\n",
    "        else:\n",
    "            delay_seconds = random.uniform(1, 3)\n",
    "            sm+=delay_seconds\n",
    "            print(sm,\" delayed \",delay_seconds)\n",
    "            print(\"chunk : \",chunk)\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk: {str(e)}\")\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "# file_name=\"pdf/14961_2016_Judgement_06-Sep-2018_cleaned_output_qa_output.xlsx\"\n",
    "# file_name = \"pdf/41070_cleaned_output_qa_output.xlsx\"\n",
    "# # Save to Excel file\n",
    "# df.to_excel(file_name, index=False)\n",
    "\n",
    "print(\"Questions and answers have been saved to qa_output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file has been saved to pdf/combined_qa_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file_1 = \"pdf/14961_2016_Judgement_06-Sep-2018_cleaned_output_qa_output.xlsx\"\n",
    "file_2 = \"pdf/41070_cleaned_output_qa_output.xlsx\"\n",
    "output_file = \"pdf/combined_qa_output.xlsx\"\n",
    "\n",
    "# Load the Excel files\n",
    "df1 = pd.read_excel(file_1)\n",
    "df2 = pd.read_excel(file_2)\n",
    "\n",
    "# Add the \"Priority\" column\n",
    "df1['Priority'] = 1\n",
    "df2['Priority'] = 0\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save to a new Excel file\n",
    "combined_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Combined file has been saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
